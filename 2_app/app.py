import streamlit as st
import langchain
from langchain.chains.question_answering import load_qa_chain
from langchain_community.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone
import openai
import os
import asyncio
from nemoguardrails import RailsConfig
from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails
from langchain_community.chat_models import ChatOpenAI

import warnings

# Suppress specific deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain_core._api.deprecation")


openai.api_key = os.getenv("OPENAI_API_KEY")

st.title('	:robot_face: :owl:  Chatbot with Guardrails')
st.caption('This Streamlit application leverages Pinecone for Semantic Search, Langchain, OpenAI and NVIDIA\'s Guardrails to demonstrate how enterprise AI can be secure and governed. Update the guardrails configuration in 2_app/config.yml')
prompt =  st.text_input('Enter your prompt')

embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

pinecone.init(
    api_key=os.getenv("PINECONE_API_KEY"),
    environment=os.getenv("PINECONE_ENVIRONMENT")
)
index_name=os.getenv("PINECONE_INDEX")
index = pinecone.Index(index_name)

# Guardrails
yaml_content=""" 
models:
- type: main
  engine: openai
  model: """ + os.getenv("OPENAI_LLM_COMPLETION_MODEL")

with open('2_app/rails.config','r') as file:
    colang_content = file.read()
  
with open('2_app/config.yml','r') as file:
    yaml_content = file.read()

# Below is another way you can reference the Rails configs
    
# config=RailsConfig.from_content(
#     colang_content=colang_content,
#     yaml_content=yaml_content
#    )
config = RailsConfig.from_path("2_app/config.yml")

model = ChatOpenAI(
    model_name=os.getenv("OPENAI_LLM_COMPLETION_MODEL"),
    openai_api_key=os.getenv("OPENAI_API_KEY"), 
    )

chain=load_qa_chain(model,chain_type="stuff")
chain_with_guardrails = RunnableRails(config, runnable=chain)

docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)
retriever = docsearch.as_retriever()

def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

# Function to handle async operations and document search
async def process_documents_and_generate_response(prompt):
    if prompt:
        question = prompt
        # Perform a similarity search to find relevant documents
        docs = docsearch.similarity_search(prompt)
        context="\n".join(docs[0].page_content)
        prompt=f""" You are a helpful assistant, below is a query from a user and some relevant context.Answer the question\
        using the information from the context. If you cannot find the answer to the question,reply with "I am sorry, I don't know".

        Context:{context}

        Query:{prompt}

        Answer: """

        response = chain.run(input_documents=docs, question=prompt)
        return response

# Execute and display the response
if prompt:  # Check if the user has entered a prompt
    with st.spinner('Fetching response...'):
        # Since the docsearch and chain operations are not inherently async,
        # we wrap this in an async function if needed or call directly if they are synchronous.
        # Adjust the call here based on your actual implementation details.
        response = asyncio.run(process_documents_and_generate_response(prompt))
        
        # Display the response generated by the QA chain
        st.write(response)



