import streamlit as st
import langchain
from langchain.chains.question_answering import load_qa_chain
from langchain_community.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone
import openai
import os
import asyncio
from nemoguardrails import RailsConfig
from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails
from langchain_community.chat_models import ChatOpenAI

from dotenv import load_dotenv

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")

st.title('	:robot_face: :owl:  Chatbot with Guardrails')
prompt =  st.text_input('Enter your prompt')

embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

pinecone.init(
    api_key=os.getenv("PINECONE_API_KEY"),
    environment=os.getenv("PINECONE_ENVIRONMENT")
)
index_name=os.getenv("PINECONE_INDEX")
index = pinecone.Index(index_name)

# Guardrails
yaml_content=""" 
models:
- type: main
  engine: openai
  model: """ + os.getenv("OPENAI_LLM_COMPLETION_MODEL")

with open('2_app/rails.config','r') as file:
    colang_content = file.read()
  
with open('2_app/config.yml','r') as file:
    yaml_content = file.read()

# Below is another way you can reference the Rails configs
    
# config=RailsConfig.from_content(
#     colang_content=colang_content,
#     yaml_content=yaml_content
#    )
config = RailsConfig.from_path("2_app/config.yml")

model = ChatOpenAI(
    model_name=os.getenv("OPENAI_LLM_COMPLETION_MODEL"),
    openai_api_key=os.getenv("OPENAI_API_KEY"), 
    )

chain=load_qa_chain(model,chain_type="stuff")
chain_with_guardrails = RunnableRails(config, runnable=chain)

docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)
retriever = docsearch.as_retriever()

def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

# Function to handle async operations and document search
async def process_documents_and_generate_response(prompt):
    if prompt:
        question = prompt
        # Perform a similarity search to find relevant documents
        docs = docsearch.similarity_search(prompt)
        context="\n".join(docs[0].page_content)
        prompt=f""" You are a helpful assistant, below is a query from a user and some relevant context.Answer the question\
        using the information from the context. If you cannot find the answer to the question,reply with "I am sorry, I don't know".

        Context:{context}

        Query:{prompt}

        Answer: """

        response = chain.run(input_documents=docs, question=prompt)
        return response

# Execute and display the response
if prompt:  # Check if the user has entered a prompt
    with st.spinner('Fetching response...'):
        # Since the docsearch and chain operations are not inherently async,
        # we wrap this in an async function if needed or call directly if they are synchronous.
        # Adjust the call here based on your actual implementation details.
        response = asyncio.run(process_documents_and_generate_response(prompt))
        
        # Display the response generated by the QA chain
        st.write(response)



